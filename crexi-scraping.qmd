---
title: "crexi-scraping"
date: "2024-08-09"
format: html
---

# Crexi Scraping

## Introduction

This document demonstrates a full workflow for web scraping, extracting and processing text from image-based PDFs, and integrating with Salesforce, ensuring no duplicate records are created.

```{python}
# Required Libraries 
import fitz 
import pytesseract 
from PIL import Image 
import io 
import requests 
import openai 
from selenium import webdriver 
from selenium.webdriver.common.by import By 
from selenium.webdriver.chrome.service import Service 
from selenium.webdriver.chrome.options import Options 
from simple_salesforce import Salesforce 
import os 
import time

# Set up your OpenAI API key

openai.api_key = 'your_openai_api_key'

# Set up your Salesforce credentials

sf = Salesforce(username='your_username', password='your_password', security_token='your_security_token')
```

# Function to extract text from image-based PDF

```{python}
def extract_text_from_pdf(pdf_path): 
  pdf_document = fitz.open(pdf_path) 
  extracted_text = ""

for page_num in range(len(pdf_document)): 
  page = pdf_document[page_num] 
  images = page.get_images(full=True)

         
  for img_index, img in enumerate(images):
    xref = img[0]
    base_image = pdf_document.extract_image(xref)
    image_bytes = base_image["image"]
    image = Image.open(io.BytesIO(image_bytes))

    text = pytesseract.image_to_string(image)
    extracted_text += text + "\n"


  pdf_document.close() 
  return extracted_text
```

# Function to process text with NLP

```{python}
def process_text_with_nlp(extracted_text): 
  response = openai.ChatCompletion.create( 
    model="gpt-4", 
    messages=[ 
      {"role": "system", "content": "Process and standardize the following text:"}, 
      {"role": "user", "content": extracted_text} 
    ] 
  ) 
  standardized_output = response['choices'][0]['message']['content'] 
  return standardized_output
```

# Function to update or insert data into Salesforce

```{python}
def upsert_salesforce_data(data, unique_field): 
  # Query Salesforce to check if the record already exists 
  existing_record = sf.query(f"SELECT Id FROM Your_Object_Name__c WHERE {unique_field} = '{data[unique_field]}'")


  if existing_record['totalSize'] > 0: 
    # Update existing record
    record_id = existing_record['records'][0]\['Id'] 
    sf.Your_Object_Name__c.update(record_id, data) 
    print(f"Updated record with {unique_field} = {data[unique_field]}") 
  else: 
    # Insert new record 
    sf.Your_Object_Name\_\_c.create(data) 
    print(f"Created new record with {unique_field} = {data\[unique_field\]}")

```

# Web scraping with Selenium

```{python}
def scrape_and_process_website(url): 
  # Set up Selenium WebDriver (e.g., ChromeDriver) 
  chrome_options = Options() 
  chrome_options.add_argument("--headless") # Run in headless mode 
  service = Service(executable_path='path/to/chromedriver') 
  driver = webdriver.Chrome(service=service, options=chrome_options)


  # Navigate to the website
  driver.get(url)

  # Example: Click on a sorting option
  sort_button = driver.find_element(By.XPATH, '//button\[contains(text(),"Sort")\]') 
  sort_button.click()

  # Example: Find links to PDFs on the page
  pdf_links = driver.find_elements(By.XPATH, '//a[contains( @href, ".pdf")]')

  for link in pdf_links: 
      pdf_url = link.get_attribute('href') 
      response = requests.get(pdf_url)
  
        
      # Save the PDF locally
      pdf_path = 'temp.pdf'
      with open(pdf_path, 'wb') as f:
         f.write(response.content)
  
    # Extract text from the PDF
    extracted_text = extract_text_from_pdf(pdf_path)
  
    # Process the text using NLP
    standardized_output = process_text_with_nlp(extracted_text)
  
    # Example data to upsert into Salesforce
    data = {
      'Your_Field_Name__c': standardized_output,  # Adjust field names and data accordingly
      'Unique_Field_Name__c': 'UniqueValue'  # Adjust based on your Salesforce schema
    }
  
    # Upsert data into Salesforce
    upsert_salesforce_data(data, 'Unique_Field_Name__c')


  # Clean up
  if os.path.exists(pdf_path): 
    os.remove(pdf_path) 
  driver.quit()

```

# Main function to run the process

```{python}
def main(): 
  # URL of the website to scrape 
  url = 'https://example.com'

  # Run the scraping and processing function
  scrape_and_process_website(url) 

  # Run the main function
  if **name** == "**main**": 
    main()
```

### **Explanation:**

1.  **YAML Front Matter**:
    -   Contains metadata such as title, author, date, and output format.
2.  **Python Code Blocks**:
    -   The Quarto document uses `{python}` to indicate Python code blocks.
    -   Each block contains the code as discussed, from importing libraries to processing data and interacting with Salesforce.
3.  **Running the Code**:
    -   When this document is rendered, it will execute the Python code and display the results.

### **Configuration Steps**:

-   **Replace API Keys and Credentials**:

    -   Update `your_openai_api_key`, `your_username`, `your_password`, and `your_security_token` with your actual credentials.

-   **Install Required Libraries**:

    -   Run the following command in your terminal or Quarto environment to install the necessary packages:

    ``` bash
    pip install pymupdf pytesseract pillow requests selenium openai simple-salesforce quarto
    ```
